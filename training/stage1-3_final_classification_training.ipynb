{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316d4539-4f3f-44f3-be6f-2771485c287d",
   "metadata": {},
   "source": [
    "## Repeat & Final stage of the training pipeline where we train our classification models to make pseudo labels and eventually final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbe70d-df7d-4614-9c6d-2cc71751085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dc61a-53dd-4dcc-86c8-2574d5336fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "def set_seed(seed = int):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    return random_state\n",
    "\n",
    "\n",
    "random_state = set_seed(1942)\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff0fee-c707-4c77-ae83-eece6a886734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"stage_f/pretrain_mlm_electra-base-turkish-cased-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f7546-2752-4712-845e-8b5a68255b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/teknofest_train_final.csv', sep='|')\n",
    "df['length'] = df['text'].apply(len)\n",
    "df = df[~(df['length']<=2)].reset_index(drop=True)\n",
    "df.loc[df['target'] == 'OTHER', 'is_offensive'] = 0\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function removes punctuation from a given text.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8f26f-afbc-45e5-933b-32908bdffb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['target'])\n",
    "\n",
    "label_to_index = {label: index for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5c74c-6fd3-40e8-b47f-390782f3749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1892323-bbed-4e39-915c-86f497704a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {v:k for k,v in label_to_index.items()}\n",
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d3068-bfee-4681-ae3a-4ab14fdbdc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13cc616-4119-4236-b848-b84d95f946f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fold'] = -1\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(df['text'], df['label'])):\n",
    "    df.loc[val_index, 'fold'] = fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60620e40-e4ea-464e-addd-409627cde7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text, label = row.text, row.label\n",
    "        encoding = tokenizer(text, max_length=64, truncation=True)\n",
    "        encoding = {key: torch.tensor(val, dtype=torch.int64) for key, val in encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return dict(encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8698581-18b8-4c98-8e6a-d6154870bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = IntentDataset(df)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee2498-61ba-42cf-841f-5c24d394869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(a[3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47dcbed-9dd5-4dcd-bb56-3b31ff36ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                               problem_type = \"single_label_classification\",\n",
    "                                                               id2label = index_to_label,\n",
    "                                                               label2id = label_to_index,                                                               \n",
    "                                                               num_labels=df.label.nunique(),     \n",
    "                                                               output_hidden_states=False, \n",
    "                                                               ignore_mismatched_sizes=True\n",
    "                                                               \n",
    "                                                          ).to('cuda'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e3839-0c47-4bd0-b7bd-ebec187dccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    score = f1_score(y_true, y_pred,\n",
    "    zero_division=0, average='macro')\n",
    "    return {\"macro f1\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcfb65-d282-4606-aed2-37fe8e317877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:    \n",
    "    N_EPOCH = 3\n",
    "    BS = 8\n",
    "    WARM_UP = 0.0\n",
    "    LR = 3e-5\n",
    "    WEIGHT_DECAY = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901926d-d6fe-44ed-95fb-54c5645ac8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = df.copy()\n",
    "oof.loc[: ,f\"pred_{model_name.split('/')[-1]}\"] = 0\n",
    "\n",
    "\n",
    "logits = np.zeros(shape=(len(oof), oof.label.nunique()))\n",
    "\n",
    "\n",
    "for fold in df.fold.unique():\n",
    "    \n",
    "    os.makedirs(f\"multiclass_{model_name.split('/')[-1]}\", exist_ok=True)\n",
    "    \n",
    "    model = model_init()\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    len_df = df[df.fold!=fold].shape[0]\n",
    "    \n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": CFG.WEIGHT_DECAY,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.LR)\n",
    "    num_training_steps = (len_df * CFG.N_EPOCH) // (CFG.BS * 1)\n",
    "    step_size = int(np.ceil((num_training_steps/CFG.N_EPOCH)/4))-1\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=CFG.WARM_UP*num_training_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        f\"turkish_profanity_{model_name.split('/')[-1]}_fold{fold}\",\n",
    "        fp16=False,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        save_strategy = \"no\",\n",
    "        learning_rate=CFG.LR,\n",
    "        per_device_train_batch_size=CFG.BS,\n",
    "        per_device_eval_batch_size=CFG.BS*2,\n",
    "        num_train_epochs=CFG.N_EPOCH,\n",
    "        # weight_decay=CFG.WEIGHT_DECAY,\n",
    "        load_best_model_at_end=False,\n",
    "        # metric_for_best_model=\"macro f1\",\n",
    "        metric_for_best_model=\"macro f1\",\n",
    "        greater_is_better=True,\n",
    "        eval_steps = step_size,\n",
    "        save_steps = step_size,\n",
    "        logging_steps = step_size,\n",
    "        seed = 1942,\n",
    "        data_seed = 1942,\n",
    "        dataloader_num_workers = 0,\n",
    "        # lr_scheduler_type =\"linear\",\n",
    "        # warmup_steps=0,               # number of warmup steps for learning rate scheduler\n",
    "        save_total_limit=2,              # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "        group_by_length = True,\n",
    "        full_determinism = True,\n",
    "        label_smoothing_factor = 0.0\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        # model_init=model_init,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=IntentDataset(df[df.fold!=fold]),\n",
    "        eval_dataset=IntentDataset(df[df.fold==fold]),\n",
    "        compute_metrics=compute_metrics,\n",
    "        # optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    preds = trainer.predict(IntentDataset(df[df.fold==fold]))\n",
    "\n",
    "    oof.loc[df.fold==fold ,f\"pred_{model_name.split('/')[-1]}\"] = np.argmax(preds.predictions, axis=1).astype(int)\n",
    "    logits[df.index[df['fold'] == fold].tolist()] += preds.predictions\n",
    "    \n",
    "\n",
    "    tokenizer.save_pretrained(f\"multiclass_{model_name.split('/')[-1]}/fold{fold}\")\n",
    "    trainer.save_model(f\"multiclass_{model_name.split('/')[-1]}/fold{fold}\")\n",
    "    \n",
    "    shutil.rmtree(f\"turkish_profanity_{model_name.split('/')[-1]}_fold{fold}\")\n",
    "    \n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "oof.to_csv(f\"multiclass_{model_name.split('/')[-1]}/multiclass_{model_name.split('/')[-1]}.csv\", index=False)\n",
    "\n",
    "np.save(f\"multiclass_{model_name.split('/')[-1]}/multiclass_{model_name.split('/')[-1]}.npy\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44734e59-b419-4893-a7f7-1b146effb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof['targ_pred'] = oof[f\"pred_{model_name.split('/')[-1]}\"].map(index_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d7a60-d8bf-4e8c-b0b2-8eec47686e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(oof.label, oof[f\"pred_{model_name.split('/')[-1]}\"], target_names=label_to_index.keys(), zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
